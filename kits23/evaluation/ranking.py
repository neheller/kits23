import sys
import shutil
import os.path
from typing import Callable, List
from pathlib import Path
import numpy as np
import scipy.stats as ss
from batchgenerators.utilities.file_and_folder_operations import isfile, join


def generate_summary_csv(list_of_folders: List[str], output_file: str = 'summary.csv'):
    """
    assumes that the folder names are the team names. Each folder must have a evaluation.csv file in it,
    generated by kits23_compute_metrics
    """
    with open(output_file, 'w') as f:
        f.write(f'TEAM_NAME,DICE_KIDNEY,DICE_MASSES,DICE_TUMOR,SDICE_KIDNEY,SDICE_MASSES,SDICE_TUMOR\n')
        for fld in list_of_folders:
            team_name = os.path.basename(fld)
            f.write(team_name)
            assert isfile(join(fld, 'evaluation.csv')), f'evaluation.csv ismissing. Run kits23_compute_metrics on folder {fld} first!'
            content = np.loadtxt(join(fld, 'evaluation.csv'), dtype=str, delimiter=',')
            last_line = content[-1]
            assert last_line[0] == 'average'
            metrics = last_line[1:].astype(float)
            for m in metrics:
                f.write(f',{m:.8f}')
            f.write('\n')


def rank_then_aggregate(data: np.ndarray, aggr_fn: Callable = np.mean):
    """
    data must be (algos x metrics). Higher values must mean better result (so Dice is OK, but not HD95!).
    If you want to use this code with HD95 you need to invert it as a preprocessing step
    :param data:
    :param aggr_fn:
    :return:
    """
    ranked = np.apply_along_axis(ss.rankdata, 0, -data, 'min')
    aggregated = aggr_fn(ranked, axis=-1)
    final_rank = ss.rankdata(aggregated, 'min')
    return final_rank, aggregated


def rank_participants(summary_csv: str, output_csv_file: str) -> None:
    results = np.loadtxt(summary_csv, dtype=str, delimiter=',', skiprows=1)
    teams = results[:, 0]

    assert len(np.unique(teams)) == len(teams), 'Some teams have identical names, please fix'
    metrics = results[:, 1:].astype(float)

    assert metrics.shape[1] == 6, 'expected 6 metrics, got %d' % metrics.shape[1]

    mean_dice = np.mean(metrics[:, :3], axis=1, keepdims=True)
    mean_sd = np.mean(metrics[:, 3:], axis=1, keepdims=True)

    mean_metrics = np.concatenate((mean_dice, mean_sd), axis=1)
    ranks, aggregated = rank_then_aggregate(mean_metrics)

    # now clean up ties. This is not the cleanest implementation, but eh
    rank = 1
    while rank < (len(teams) + 1):
        num_teams_on_that_rank = sum(ranks == rank)
        if num_teams_on_that_rank <= 1:
            rank += 1
            continue
        else:
            # tumor dice is tie breaker
            teams_mask = ranks == rank
            tumor_dice_scores = metrics[teams_mask, 2:3]
            new_ranks = rank_then_aggregate(tumor_dice_scores)[0] - 1 + rank
            if len(np.unique(new_ranks)) == 1:
                print("WARNING: Cannot untie ranks of these teams... tumor_dice_scores and ranks are identical:")
                print('team names:', teams[teams_mask])
                rank += 1
            if len(np.unique(new_ranks)) != sum(teams_mask):
                ranks[teams_mask] = new_ranks
                continue
            ranks[teams_mask] = new_ranks
            rank += 1

    # print ranking
    sorting = np.argsort(ranks)
    with open(output_csv_file, 'w') as f:
        f.write('team_name,final_rank,mean_rank,mean_dice,mean_sd,tumor_dice\n')
        for i in sorting:
            f.write('%s,%d,%.4f,%.8f,%.8f,%.8f\n' % (teams[i], ranks[i], aggregated[i], mean_dice[i], mean_sd[i],
                                                     metrics[i, 2]))


def find_folder_with_eval(sub_folder_pth):
    # Iterate through directories to find folder with submission files
    if len(list(sub_folder_pth.glob("*evaluation.csv*"))) == 1:
        return sub_folder_pth
    
    for child in sub_folder_pth.glob("*"):
        if not child.is_dir():
            pass
        
        candidate = find_folder_with_eval(child)
        if candidate is not None:
            return candidate

    return None


if __name__ == '__main__':
    # Create temp folder to store all evaluation.csv
    tmp_pth = Path(__file__).parent / ".submissions"
    tmp_pth.mkdir(exist_ok=True)

    all_folders = []
    all_submission_pth = Path(sys.argv[1])
    for submission_parent in all_submission_pth.glob("*"):
        if not submission_parent.is_dir():
            continue
        
        submission_pth = find_folder_with_eval(submission_parent)
        tmp_sub_pth = tmp_pth / submission_parent.name
        tmp_sub_pth.mkdir(exist_ok=True)
        shutil.copy(str(submission_pth / "evaluation.csv"), tmp_sub_pth)
        all_folders.append(str(tmp_sub_pth))

    # Create folder with results
    results_pth = Path(__file__).parent / ".results"
    results_pth.mkdir(exist_ok=True)
    summary_file = str(results_pth / "summary.csv")
    output_file = str(results_pth / "kits2023_ranking.csv")

    # Generate summary.csv then rank
    generate_summary_csv(all_folders, summary_file)
    rank_participants(summary_file, output_file)